{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Interactive Analysis in Tornasole\n",
    "This notebook will demonstrate the simplest kind of interactive analysis that can be run in smdebug. It will focus on the [vanishing/exploding gradient](https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb) problems on a simple MNIST digit recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic setup that's always helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that MXNet is accessible! If you are on the EC2 Deep Learning AMI, you will probably want\n",
    "to activate the right MXNet environment\n",
    "```\n",
    "sh> source activate mxnet_p36\n",
    "```\n",
    "You'll probably have to restart this notebook after doing this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import some basic libraries for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's copy the Tornasole libraries to this instance, this step has to be executed only once. \n",
    "Please make sure that the AWS account you are using can access the `tornasole-external-preview-use1` bucket.\n",
    "\n",
    "To do so you'll need the appropriate AWS credentials. There are several ways of doing this:\n",
    "- inject temporary credentials \n",
    "- if running on EC2, use [EC2 roles](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html) that can access all S3 buckets\n",
    "- (preferred) run this notebook on a [SageMaker notebook instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html)\n",
    "\n",
    "The code below downloads the necessary `.whl` files and installs them in the current environment. Only run the first time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING - uncomment this code only if you haven't done this before\n",
    "#!aws s3 sync s3://tornasole-external-preview-use1/sdk/ts-binaries/tornasole_mxnet/py3/latest/ tornasole_mxnet/\n",
    "#!pip install tornasole_mxnet/*\n",
    "\n",
    "# If you run into a version conflict with boto, run the following\n",
    "#!pip uninstall -y botocore boto3 aioboto3 aiobotocore && pip install botocore==1.12.91 boto3==1.9.91 aiobotocore==0.10.2 aioboto3==6.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model Training and Gradient Analysis\n",
    "At this point we have all the ingredients installed on our machine. We can now start training.\n",
    "\n",
    "The goal of this notebook is to show how to detect the Vanishing Gradient problem. We will first do it manually and then automatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.mxnet import SessionHook, SaveConfig\n",
    "from smdebug.trials import LocalTrial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the logging level if appropriate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import logging\n",
    "#logging.getLogger(\"tornasole\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a simple network - it doesn't really matter what it is.\n",
    "Importantly - we **add the Tornasole Hook**. This hook will be run at every batch and will save selected tensors (in this case, all of them) to the desired directory (in this case, `'{base_loc}/{run_id}'`.\n",
    "\n",
    "`{base_loc}` can be either a path on a local file system (for instance, `./ts_output/`) or an S3 bucket/object (`s3://mybucket/myprefix/`).\n",
    "\n",
    "See the documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smdebug.mxnet as smd\n",
    "def create_net( tornasole_save_interval, base_loc, run_id ):\n",
    "    net = nn.Sequential(prefix='sequential_')\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Dense(128, activation='relu'))\n",
    "        net.add(nn.Dense(64, activation='relu'))\n",
    "        net.add(nn.Dense(10))\n",
    "\n",
    "    # Create and add the hook. Arguments:\n",
    "    # - save data in './{base_loc}/{run_id} - Note: s3 is also supported\n",
    "    # - save every 100 batches\n",
    "    # - save every tensor: inputs/outputs to each layer, as well as gradients\n",
    "    trial_dir = base_loc + run_id\n",
    "    hook = SessionHook(out_dir=trial_dir,\n",
    "                     save_config=SaveConfig(save_interval=100), \n",
    "                     save_all=True)\n",
    "    hook.register_hook(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create a simple training script. No Tornasole-specific code here, this is a slightly modified version of the [digit recognition](https://github.com/apache/incubator-mxnet/blob/master/example/gluon/mnist/mnist.py) example on the MXNet website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(data, label):\n",
    "    data = data.reshape((-1,)).astype(np.float32)/255\n",
    "    return data, label\n",
    "\n",
    "def test(ctx, val_data):\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for data, label in val_data:\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        metric.update([label], [output])\n",
    "    return metric.get()\n",
    "\n",
    "def train(net, epochs, ctx, learning_rate, momentum):\n",
    "    train_data = gluon.data.DataLoader(\n",
    "        gluon.data.vision.MNIST('./data', train=True, transform=transformer),\n",
    "        batch_size=100, shuffle=True, last_batch='discard')\n",
    "\n",
    "    val_data = gluon.data.DataLoader(\n",
    "        gluon.data.vision.MNIST('./data', train=False, transform=transformer),\n",
    "        batch_size=100, shuffle=False)\n",
    "    \n",
    "    # Collect all parameters from net and its children, then initialize them.\n",
    "    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "    # Trainer is for updating parameters with gradient.\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\n",
    "                            {'learning_rate': learning_rate, 'momentum': momentum})\n",
    "    metric = mx.metric.Accuracy()\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # reset data iterator and metric at begining of epoch.\n",
    "        metric.reset()\n",
    "        for i, (data, label) in enumerate(train_data):\n",
    "            # Copy data to ctx if necessary\n",
    "            data = data.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            # Start recording computation graph with record() section.\n",
    "            # Recorded graphs can then be differentiated with backward.\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                L = loss(output, label)\n",
    "                L.backward()\n",
    "            # take a gradient step with batch_size equal to data.shape[0]\n",
    "            trainer.step(data.shape[0])\n",
    "            # update metric at last.\n",
    "            metric.update([label], [output])\n",
    "\n",
    "            if i % 100 == 0 and i > 0:\n",
    "                name, acc = metric.get()\n",
    "                print('[Epoch %d Batch %d] Training: %s=%f'%(epoch, i, name, acc))\n",
    "\n",
    "        name, acc = metric.get()\n",
    "        print('[Epoch %d] Training: %s=%f'%(epoch, name, acc))\n",
    "\n",
    "        name, val_acc = test(ctx, val_data)\n",
    "        print('[Epoch %d] Validation: %s=%f'%(epoch, name, val_acc))\n",
    "\n",
    "    net.save_parameters('mnist.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear up from previous runs, we remove old data (warning - we assume that we have set `ts_output` as the directory into which we send data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./ts_output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are ready to train. We will train this simple model.\n",
    "\n",
    "For the purposes of this example, we will name this run as `'good'` because we know it will converge to a good solution.  If you have a GPU on your machine, you can change `ctx=mx.gpu(0)`.\n",
    "\n",
    "Behind the scenes, the `SessionHook` is saving the data requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_net( tornasole_save_interval=100, base_loc='./ts_output/', run_id='good')\n",
    "train(net=net, epochs=4, ctx=mx.cpu(), learning_rate=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis - Manual\n",
    "Now that we have trained the system we can analyze the data. Notice that this notebook focuses on after-the-fact analysis. Tornasole also provides a collection of tools to do automatic analysis as the training run is progressing, which will be covered in a different notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import a basic analysis library, which defines a concept of `Trial`. A `Trial` is a single training run, which is depositing values in a local directory (`LocalTrial`) or S3 (`S3Trial`). In this case we are using a `LocalTrial` - if you wish, you can change the output from `./ts_output` to `s3://mybucket/myprefix` and use `S3Trial` instead of `LocalTrial`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_trial = LocalTrial( 'myrun', './ts_output/good/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list all the tensors we know something about. Each one of these names is the name of a tensor - the name is a combination of the layer name (which, in these cases, is auto-assigned by MXNet) and whether it's an input/output/gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_trial.tensornames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting feature is *collections*. Collection represents a set of tensors that are grouped together by a condition. See [more docs on Collections](https://github.com/awslabs/tornasole_core/blob/alpha/docs/mxnet/api.md#collection). For example, here we can inspect which tensors got into collection named '*gradients*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_trial.tensors_in_collection('gradients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each tensor we can ask for which steps we have data - in this case, every 100 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_trial.tensor('gradient/sequential_dense0_weight').steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain each tensor at each step as a `numpy` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(good_trial.tensor('gradient/sequential_dense0_weight').value(300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a simple function that prints the `np.mean` of the `np.abs` of each gradient. We expect each gradient to get smaller over time, as the system converges to a good solution. Now, remember that this is an interactive analysis - we are showing these tensors to give an idea of the data. \n",
    "\n",
    "Later on in this notebook we will run an automated analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that, for the given tensor name, walks through all \n",
    "# the batches for which we have data and computes mean(abs(tensor)).\n",
    "# Returns the set of steps and the values\n",
    "\n",
    "def get_data(trial, tname):\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps()\n",
    "    vals = []\n",
    "    for s in steps:\n",
    "        val = tensor.value(s)\n",
    "        val = np.mean(np.abs(val))\n",
    "        vals.append(val)\n",
    "    return steps, vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradients( lt ):\n",
    "    for tname in lt.tensornames():\n",
    "        if not 'gradient' in tname: continue\n",
    "        steps, data = get_data(lt, tname)\n",
    "        plt.plot( steps, data, label=tname)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these gradiends. Notice how they are (mostly!) decreasing. We should investigate the spikes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradients(good_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print inputs and outputs from the model. For instance, let's print the 83th sample of the 2700th batch, as seen by the network. \n",
    "\n",
    "Notice that we have to reshape the input data from a (784,) array to a (28,28) array and multiply by 255 - the exact inverse of the transformation we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw tensor\n",
    "raw_t = good_trial.tensor('sequential_input_0').value(2700)[83]\n",
    "# We have to undo the transformations in 'transformer' above. First of all, multiply by 255\n",
    "raw_t = raw_t * 255\n",
    "# Then reshape from a 784-long vector to a 28x28 square.\n",
    "input_image = raw_t.reshape(28,28)\n",
    "plt.imshow(input_image, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the relative values emitted by the network. Notice that the last layer is of type `Dense(10)`: it will emit 10 separate confidences, one for each 0-9 digit. The one with the highest output is the predicted value.\n",
    "\n",
    "We can capture and plot the network output for the same sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(good_trial.tensor('sequential_output0').value(2700)[83], 'bo')\n",
    "plt.show()\n",
    "print( 'The network predicted the value: {}'.format(np.argmax(good_trial.tensor('sequential_output0').value(2700)[83])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now worked through some of the basics. Let's pretend we are debugging a real problem: the [Vanishing Gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). When training a network, if the `learning_rate` is too high we will end up with a Vanishing Gradient. Let's set `learning_rate=1`.\n",
    "\n",
    "Notice how the accuracy remains at around ~10% - no better than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_net( tornasole_save_interval=100, base_loc='./ts_output/', run_id='bad')\n",
    "train(net=net, epochs=4, ctx=mx.cpu(), learning_rate=1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_trial = LocalTrial( 'myrun', './ts_output/bad/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the gradients - notice how every single one of them (apart from one) goes to zero and stays there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradients(bad_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `VanishingGradient` rule provided by Tornasole alerts for this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = (bad_trial.tensor('sequential_input_0').value(2700)[83]*255).reshape(28,28)\n",
    "plt.imshow(input_image, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bad_trial.tensor('sequential_output0').value(2700)[83], 'bo')\n",
    "plt.show()\n",
    "print( 'The network predicted the value: {}'.format(np.argmax(bad_trial.tensor('sequential_output0').value(2700)[83])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis - Automatic\n",
    "So far we have conducted a human analysis, but the real power of Tornasole comes from having automatic monitoring of training runs. To do so we will build a SageMaker-based system that monitors existing runs in real time. Data traces deposited in S3 are the exchange mechanism: \n",
    "- the training system deposits data into s3://mybucket/myrun/\n",
    "- the monitoring system watches and reads data from s3://mybucket/myrun/\n",
    "\n",
    "In this example we will simulate that situation. The only difference from SageMaker-based system is that data traces have been stored locally, not in S3, so we will use previously created `LocalTrial` objects and run rule on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.rules.generic import VanishingGradient\n",
    "from smdebug.rules.rule_invoker import invoke_rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = VanishingGradient(base_trial=good_trial, threshold=0.0001)\n",
    "invoke_rule(vr, end_step=2700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_bad = VanishingGradient(base_trial=bad_trial, threshold=0.0001)\n",
    "invoke_rule(vr_bad, end_step=2700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this notebook. For more information see the documentation at  \n",
    "- https://github.com/awslabs/tornasole_core\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
